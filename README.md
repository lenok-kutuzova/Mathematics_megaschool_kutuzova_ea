# Mathematics_megaschool_kutuzova_ea

## Здравствуйте, уважаемые преподаватели и эксперты!

Представляю вам небольшой вывод по проделанной работе и научный выод о λ=2.35.

В работе мной была построена среда навигации дрона с заданными условиями успеха (радиус, заряд, ограничение по шагам) и обучены два табличных агента: классический Q‑Learning и поведенческая модель, построенная на теории перспектив Канемана и Тверски (Prospect Theory Agent). Оба агента обучались 15000 эпизодов с одинаковыми гиперпараметрами, но ни один из них фактически не достиг успеха.

Классический Q‑Learning за 15000 эпизодов обучения показывает среднюю награду порядка −144 за эпизод и 56 успешных эпизодов, что отражает очень жёсткую постановку задачи и редкую награду. В испытаниях (300 тестовых эпизодов с почти жадной политикой, ε≈0.005) классический Q‑агент также ни разу формально не достигает успеха (доля успешных эпизодов 0%), среднее число шагов до завершения около 180 (эпизоды в основном упираются в таймаут), средняя потраченная энергия порядка 0.35, среднее финальное значение батареи около 0.15, среднее расстояние до станции по окончании примерно 7.1 метра, а средняя суммарная награда за эпизод остаётся около −144.35.
​
Prospect Theory‑агент с λ=2.35 демонстрирует схожую картину: успехов также 0%, шаги до завершения близки к 180, но он в среднем тратит меньше энергии (≈0.48 против 0.35 у Q‑Learning), заканчивает эпизоды с более высоким остаточным зарядом (≈0.48 против 0.35 по батарее) и чуть меньшим расстоянием до станции (≈5.7 м против ≈7.1 м), при этом его средняя награда немного выше (−144.03 против −144.35).
​
Сравнительные графики и t‑тесты показывают статистически значимые, но по абсолютной величине небольшие различия по затратам энергии, финальному заряду и расстоянию до станции, однако ни один из агентов в текущей конфигурации среды не достигает приемлемого уровня успешных эпизодов.
​
_______________________________________________________________________________________________

## Научный вывод о λ=2.35

В агенте теории перспектив используется параметр λ=2.35, отвечающий за неприятие потерь в value‑функции: выигрыши оцениваются с показателем α=0.88, потери — β=0.88 и умножаются на λ, что дицелает отрательные исходы субъективно в 2.35 раза "более болезненными", чем равные по модулю выигрыши. Параметр λ=2.35 находится очень близко к классическим оценкам Тверски и Канемана (около 2–2.25) и означает умеренное, "человеко‑подобное" неприятие риска: агент сильнее реагирует на эпизоды с провалом (разрядка, столкновение, большой штраф по шагам), чем на редкие относительно хорошие исходы. 

Однако в конкретной среде такая степень неприятия потерь в сочетании с труднодостижимой большой наградой за успех и постоянными шаговыми штрафами фактически усиливает консервативное поведение: агент ещё сильнее "боится" ухудшения и не идёт на рискованные траектории для захода на станцию, в результате его доля успехов остаётся нулевой и не превосходит рационального Q‑агента.
​
С научной точки зрения это показывает, что введение поведенческого параметра λ>1 само по себе не гарантирует улучшения эксплуатационных метрик в сложной навигационной задаче: при фиксированной среде и награде увеличение неприятия потерь может даже препятствовать обучению рискуемой, но целевой стратегии, если редкая большая награда почти никогда не наблюдается в данных.
​
Следовательно, λ = 2.35 в данной постановке корректно реализует поведенческую черту «избегание потерь», но при текущих настройках среды она не приводит к измеримому выигрышу над классическим Q‑Learning и подчёркивает, что в задачах с подобной структурой награды решающими факторами остаются сама формулировка и разбиение награды, а также выбор алгоритма обучения, а не только конкретный вид функции полезности.​
